{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios de Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Utilizaremos os dados de um [distribuidor por atacado Português](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers) para agrupamento. Essa base de dados tem o nome de `Wholesale_Customers_Data`.\n",
    "\n",
    "Essa base contêm os seguintes atributos:\n",
    "\n",
    "* Fresh: annual spending (m.u.) on fresh products\n",
    "* Milk: annual spending (m.u.) on milk products\n",
    "* Grocery: annual spending (m.u.) on grocery products\n",
    "* Frozen: annual spending (m.u.) on frozen products\n",
    "* Detergents_Paper: annual spending (m.u.) on detergents and paper products\n",
    "* Delicatessen: annual spending (m.u.) on delicatessen products\n",
    "* Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n",
    "* Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n",
    "\n",
    "Nessa base, os valores de gastos são dados em uma unidade arbitrária (m.u. = monetary unit, unidade monetária)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "\n",
    "* Importe os dados e verifique os seus tipos.\n",
    "* Remova as colunas channel e region.\n",
    "* Converta o restante para floats se necessário.\n",
    "* Copie a variável original de data (usando o método `copy`) para preservá-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = 'data/Wholesale_Customers_Data.csv'\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "copy = copy.copy(data)\n",
    "\n",
    "data = data.drop(['Channel','Region'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fresh               int64\n",
       "Milk                int64\n",
       "Grocery             int64\n",
       "Frozen              int64\n",
       "Detergents_Paper    int64\n",
       "Delicassen          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tipos das variáveis\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converte para floats\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça uma cópia do original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-390b5ae0a98b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   3430\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m         \"\"\"\n\u001b[0;32m-> 3432\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3433\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m   3425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3426\u001b[0m         \u001b[0;31m# this preserves the notion of view copying of axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3427\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3429\u001b[0m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m    954\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "data_orig = copy.copy(copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "\n",
    "Vamos repetir o procedimento de transformação e escala da atividade anterior (Exercício 2):\n",
    "\n",
    "* Examine a correlação e viés.\n",
    "* Faça a transformação e escalonamento necessários.\n",
    "* Visualize as correlações par a par."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matriz de correlação\n",
    "corr_mat = data.???\n",
    "\n",
    "# Sete a diagonal com 0.0\n",
    "for x in range(corr_mat.shape[0]):\n",
    "    corr_mat.iloc[x,x] = ???\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retorne o id de máxima correlação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_mat.???.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine os atributos com viés e aplique log1p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_columns = data.???.sort_values(ascending=False)\n",
    "log_columns = log_columns.loc[log_columns > 0.75]\n",
    "\n",
    "log_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The log transformations\n",
    "for col in log_columns.index:\n",
    "    data[col] = ???(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar o `MinMaxScaler` para fazer o escalonamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col] = mms.fit_transform(???).squeeze() # squeeze transforma em vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar as relações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.set_palette('dark')\n",
    "sns.set_style('white')\n",
    "\n",
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "\n",
    "* Usando a função [pipeline](http://scikit-learn.org/stable/modules/pipeline.html) do Scikit-Learn, recrie o procedimento acima de pré-processamento (transformação e escala) usando pipeline. Para utilizar uma função de transformação externa (log1p do Numpy) você precisará da classe chamada [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers).\n",
    "* Use o pipeline para transformar a cópia dos dados originais.\n",
    "* Compare com o resultado do exercício anterior para verificar se funcionou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Crie a transformação log do Numpy para ser utilizada no pipeline\n",
    "log_transformer = FunctionTransformer(???)\n",
    "\n",
    "# O pipeline\n",
    "estimators = [('log1p', log_transformer), ('minmaxscale', MinMaxScaler())]\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Converte a cópia dos dados originais\n",
    "data_pipe = pipeline.fit_transform(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados são idênticos. Note que podemos acrescentar qualquer modelo dentro do pipeline, como algoritmos de classificação e regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.allclose(data_pipe, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "\n",
    "* Aplique o PCA com `n_components` variando entre 1 e 5. \n",
    "* Armazene a quantidade de variância explicada (explained_variance_ratio_.sum()) para cada número de dimensões.\n",
    "* Vamos estimar a importância dos atributos gerados com np.abs(components_).sum(axis=0)/ soma de todos.\n",
    "* Plote os valores acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_list = list()\n",
    "feature_weight_list = list()\n",
    "\n",
    "# Fit a range of PCA models\n",
    "\n",
    "for n in range(???, ???):\n",
    "    \n",
    "    # Create and fit the model\n",
    "    PCAmod = PCA(n_components=???)\n",
    "    PCAmod.fit(???)\n",
    "    \n",
    "    # Store the model and variance\n",
    "    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n",
    "                               'var': PCAmod.explained_variance_ratio_.sum()}))\n",
    "    \n",
    "    # Calculate and store feature importances\n",
    "    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n",
    "    feature_weight_list.append(pd.DataFrame({'n':n, \n",
    "                                             'features': data.columns,\n",
    "                                             'values':abs_feature_values/abs_feature_values.sum()}))\n",
    "    \n",
    "pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma tabela para verificar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df = (pd.concat(feature_weight_list)\n",
    "               .pivot(index='n', columns='features', values='values'))\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos plotar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = pca_df['var'].plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Percent explained variance',\n",
    "       title='Explained Variance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E da importância dos atributos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = features_df.plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Relative importance',\n",
    "       title='Feature importance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5\n",
    "\n",
    "Vamos verificar como a acurácia de nosso modelo pode mudar se incluírmos um `PCA` no nosso pipeline de processamento. Vamos criar um pipeline com os seguintes passos:\n",
    "<ol>\n",
    "  <li>Escalonar</li>\n",
    "  <li>`PCA(n_components=n)`</li>\n",
    "  <li>`LogisticRegression`</li>\n",
    "</ol>\n",
    "\n",
    "* Para isso utilizaremos a base de dados Human Activity.\n",
    "* Escreva uma função que pega um valor  `n`  e constrói o pipeline acima, faça a predição da coluna  \"Activity\" em um StratifiedShuffleSplit de 5 pastas (fold), retorne a acurácia média da base de teste.\n",
    "* Chame a função acima para vários valores de n.\n",
    "* Plote a acurácia média pela dimensão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = 'data/Human_Activity_Recognition_Using_Smartphones_Data.csv'\n",
    "data2 = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = data2.drop('Activity', axis=1)\n",
    "y = data2.Activity\n",
    "sss = StratifiedShuffleSplit(n_splits=???, random_state=42)\n",
    "\n",
    "def get_avg_score(n):\n",
    "    pipe = [\n",
    "        ('scaler', ???),\n",
    "        ('pca', ???),\n",
    "        ('estimator', ???)\n",
    "    ]\n",
    "    pipe = Pipeline(pipe)\n",
    "    scores = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        pipe.fit(???, ???)\n",
    "        scores.append(accuracy_score(???, pipe.predict(???)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "ns = [10, 20, 50, 100, 150, 200, 300, 400]\n",
    "score_list = [get_avg_score(n) for n in ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(ns, score_list)\n",
    "ax.set(xlabel='Number of Dimensions',\n",
    "       ylabel='Average Accuracy',\n",
    "       title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
