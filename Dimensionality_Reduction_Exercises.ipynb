{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios de Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Utilizaremos os dados de um [distribuidor por atacado Português](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers) para agrupamento. Essa base de dados tem o nome de `Wholesale_Customers_Data`.\n",
    "\n",
    "Essa base contêm os seguintes atributos:\n",
    "\n",
    "* Fresh: annual spending (m.u.) on fresh products\n",
    "* Milk: annual spending (m.u.) on milk products\n",
    "* Grocery: annual spending (m.u.) on grocery products\n",
    "* Frozen: annual spending (m.u.) on frozen products\n",
    "* Detergents_Paper: annual spending (m.u.) on detergents and paper products\n",
    "* Delicatessen: annual spending (m.u.) on delicatessen products\n",
    "* Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n",
    "* Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n",
    "\n",
    "Nessa base, os valores de gastos são dados em uma unidade arbitrária (m.u. = monetary unit, unidade monetária)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "\n",
    "* Importe os dados e verifique os seus tipos.\n",
    "* Remova as colunas channel e region.\n",
    "* Converta o restante para floats se necessário.\n",
    "* Copie a variável original de data (usando o método `copy`) para preservá-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = 'data/Wholesale_Customers_Data.csv'\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "copia = copy.copy(data)\n",
    "\n",
    "data = data.drop(['Channel','Region'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fresh               int64\n",
       "Milk                int64\n",
       "Grocery             int64\n",
       "Frozen              int64\n",
       "Detergents_Paper    int64\n",
       "Delicassen          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tipos das variáveis\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converte para floats\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça uma cópia do original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = copy.copy(copia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "\n",
    "Vamos repetir o procedimento de transformação e escala da atividade anterior (Exercício 2):\n",
    "\n",
    "* Examine a correlação e viés.\n",
    "* Faça a transformação e escalonamento necessários.\n",
    "* Visualize as correlações par a par."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fresh</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100510</td>\n",
       "      <td>-0.011854</td>\n",
       "      <td>0.345881</td>\n",
       "      <td>-0.101953</td>\n",
       "      <td>0.244690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milk</th>\n",
       "      <td>0.100510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.728335</td>\n",
       "      <td>0.123994</td>\n",
       "      <td>0.661816</td>\n",
       "      <td>0.406368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grocery</th>\n",
       "      <td>-0.011854</td>\n",
       "      <td>0.728335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.040193</td>\n",
       "      <td>0.924641</td>\n",
       "      <td>0.205497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frozen</th>\n",
       "      <td>0.345881</td>\n",
       "      <td>0.123994</td>\n",
       "      <td>-0.040193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.131525</td>\n",
       "      <td>0.390947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <td>-0.101953</td>\n",
       "      <td>0.661816</td>\n",
       "      <td>0.924641</td>\n",
       "      <td>-0.131525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delicassen</th>\n",
       "      <td>0.244690</td>\n",
       "      <td>0.406368</td>\n",
       "      <td>0.205497</td>\n",
       "      <td>0.390947</td>\n",
       "      <td>0.069291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Fresh      Milk   Grocery    Frozen  Detergents_Paper  \\\n",
       "Fresh             0.000000  0.100510 -0.011854  0.345881         -0.101953   \n",
       "Milk              0.100510  0.000000  0.728335  0.123994          0.661816   \n",
       "Grocery          -0.011854  0.728335  0.000000 -0.040193          0.924641   \n",
       "Frozen            0.345881  0.123994 -0.040193  0.000000         -0.131525   \n",
       "Detergents_Paper -0.101953  0.661816  0.924641 -0.131525          0.000000   \n",
       "Delicassen        0.244690  0.406368  0.205497  0.390947          0.069291   \n",
       "\n",
       "                  Delicassen  \n",
       "Fresh               0.244690  \n",
       "Milk                0.406368  \n",
       "Grocery             0.205497  \n",
       "Frozen              0.390947  \n",
       "Detergents_Paper    0.069291  \n",
       "Delicassen          0.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matriz de correlação\n",
    "corr_mat = data.corr()\n",
    "\n",
    "# Sete a diagonal com 0.0\n",
    "for x in range(corr_mat.shape[0]):\n",
    "    corr_mat.iloc[x,x] = 0.0\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retorne o id de máxima correlação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fresh               1.0\n",
       "Milk                1.0\n",
       "Grocery             1.0\n",
       "Frozen              1.0\n",
       "Detergents_Paper    1.0\n",
       "Delicassen          1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mat.corr().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine os atributos com viés e aplique log1p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181    112151.0\n",
       "125     76237.0\n",
       "284     68951.0\n",
       "39      56159.0\n",
       "258     56083.0\n",
       "103     56082.0\n",
       "259     53205.0\n",
       "282     49063.0\n",
       "239     47493.0\n",
       "176     45640.0\n",
       "47      44466.0\n",
       "87      43265.0\n",
       "29      43088.0\n",
       "289     42786.0\n",
       "129     42312.0\n",
       "52      40721.0\n",
       "285     40254.0\n",
       "370     39679.0\n",
       "436     39228.0\n",
       "377     38793.0\n",
       "142     37036.0\n",
       "183     36847.0\n",
       "273     36817.0\n",
       "124     36050.0\n",
       "61      35942.0\n",
       "382     34454.0\n",
       "325     32717.0\n",
       "149     31812.0\n",
       "12      31714.0\n",
       "255     31614.0\n",
       "         ...   \n",
       "327       542.0\n",
       "392       518.0\n",
       "173       514.0\n",
       "272       514.0\n",
       "98        503.0\n",
       "53        491.0\n",
       "299       444.0\n",
       "97        403.0\n",
       "340       381.0\n",
       "159       355.0\n",
       "184       327.0\n",
       "174       286.0\n",
       "170       260.0\n",
       "342       255.0\n",
       "305       243.0\n",
       "81        219.0\n",
       "171       200.0\n",
       "355       190.0\n",
       "193       180.0\n",
       "304       161.0\n",
       "128       140.0\n",
       "353       117.0\n",
       "412        97.0\n",
       "65         85.0\n",
       "357        37.0\n",
       "96         23.0\n",
       "218        18.0\n",
       "66          9.0\n",
       "338         3.0\n",
       "95          3.0\n",
       "Name: Fresh, Length: 440, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_columns = data.Fresh.sort_values(ascending=False)\n",
    "log_columns = log_columns.loc[log_columns > 0.75]\n",
    "\n",
    "log_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-877d673f2a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The log transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    }
   ],
   "source": [
    "# The log transformations\n",
    "for col in log_columns.index:\n",
    "    data[col] = fit(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar o `MinMaxScaler` para fazer o escalonamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col] = mms.fit_transform(???).squeeze() # squeeze transforma em vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar as relações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.set_palette('dark')\n",
    "sns.set_style('white')\n",
    "\n",
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "\n",
    "* Usando a função [pipeline](http://scikit-learn.org/stable/modules/pipeline.html) do Scikit-Learn, recrie o procedimento acima de pré-processamento (transformação e escala) usando pipeline. Para utilizar uma função de transformação externa (log1p do Numpy) você precisará da classe chamada [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers).\n",
    "* Use o pipeline para transformar a cópia dos dados originais.\n",
    "* Compare com o resultado do exercício anterior para verificar se funcionou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Crie a transformação log do Numpy para ser utilizada no pipeline\n",
    "log_transformer = FunctionTransformer(???)\n",
    "\n",
    "# O pipeline\n",
    "estimators = [('log1p', log_transformer), ('minmaxscale', MinMaxScaler())]\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Converte a cópia dos dados originais\n",
    "data_pipe = pipeline.fit_transform(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados são idênticos. Note que podemos acrescentar qualquer modelo dentro do pipeline, como algoritmos de classificação e regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.allclose(data_pipe, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "\n",
    "* Aplique o PCA com `n_components` variando entre 1 e 5. \n",
    "* Armazene a quantidade de variância explicada (explained_variance_ratio_.sum()) para cada número de dimensões.\n",
    "* Vamos estimar a importância dos atributos gerados com np.abs(components_).sum(axis=0)/ soma de todos.\n",
    "* Plote os valores acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_list = list()\n",
    "feature_weight_list = list()\n",
    "\n",
    "# Fit a range of PCA models\n",
    "\n",
    "for n in range(0, len(pca_list)):\n",
    "    \n",
    "    # Create and fit the model\n",
    "    PCAmod = PCA(n_components=feature_weight_list)\n",
    "    PCAmod.fit(pca_list)\n",
    "    \n",
    "    # Store the model and variance\n",
    "    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n",
    "                               'var': PCAmod.explained_variance_ratio_.sum()}))\n",
    "    \n",
    "    # Calculate and store feature importances\n",
    "    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n",
    "    feature_weight_list.append(pd.DataFrame({'n':n, \n",
    "                                             'features': data.columns,\n",
    "                                             'values':abs_feature_values/abs_feature_values.sum()}))\n",
    "    \n",
    "pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma tabela para verificar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df = (pd.concat(feature_weight_list)\n",
    "               .pivot(index='n', columns='features', values='values'))\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos plotar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = pca_df['var'].plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Percent explained variance',\n",
    "       title='Explained Variance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E da importância dos atributos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = features_df.plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Relative importance',\n",
    "       title='Feature importance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5\n",
    "\n",
    "Vamos verificar como a acurácia de nosso modelo pode mudar se incluírmos um `PCA` no nosso pipeline de processamento. Vamos criar um pipeline com os seguintes passos:\n",
    "<ol>\n",
    "  <li>Escalonar</li>\n",
    "  <li>`PCA(n_components=n)`</li>\n",
    "  <li>`LogisticRegression`</li>\n",
    "</ol>\n",
    "\n",
    "* Para isso utilizaremos a base de dados Human Activity.\n",
    "* Escreva uma função que pega um valor  `n`  e constrói o pipeline acima, faça a predição da coluna  \"Activity\" em um StratifiedShuffleSplit de 5 pastas (fold), retorne a acurácia média da base de teste.\n",
    "* Chame a função acima para vários valores de n.\n",
    "* Plote a acurácia média pela dimensão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = 'data/Human_Activity_Recognition_Using_Smartphones_Data.csv'\n",
    "data2 = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = data2.drop('Activity', axis=1)\n",
    "y = data2.Activity\n",
    "sss = StratifiedShuffleSplit(n_splits=???, random_state=42)\n",
    "\n",
    "def get_avg_score(n):\n",
    "    pipe = [\n",
    "        ('scaler', ???),\n",
    "        ('pca', ???),\n",
    "        ('estimator', ???)\n",
    "    ]\n",
    "    pipe = Pipeline(pipe)\n",
    "    scores = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        pipe.fit(???, ???)\n",
    "        scores.append(accuracy_score(???, pipe.predict(???)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "ns = [10, 20, 50, 100, 150, 200, 300, 400]\n",
    "score_list = [get_avg_score(n) for n in ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(ns, score_list)\n",
    "ax.set(xlabel='Number of Dimensions',\n",
    "       ylabel='Average Accuracy',\n",
    "       title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
